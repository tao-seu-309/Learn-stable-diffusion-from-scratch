{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weight: tensor([[[0.4809, 0.5191],\n",
      "         [0.4735, 0.5265]],\n",
      "\n",
      "        [[0.5140, 0.4860],\n",
      "         [0.5096, 0.4904]],\n",
      "\n",
      "        [[0.5013, 0.4987],\n",
      "         [0.5014, 0.4986]]], grad_fn=<SoftmaxBackward0>)\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "attention_weight: tensor([[[0.4809, 0.5191],\n",
      "         [0.4735, 0.5265]],\n",
      "\n",
      "        [[0.5140, 0.4860],\n",
      "         [0.5096, 0.4904]],\n",
      "\n",
      "        [[0.5013, 0.4987],\n",
      "         [0.5014, 0.4986]]])\n",
      "SelfAttV1 FLOPs: 0.000288 MFLOPs\n",
      "SelfAttV1 Parameters: 0.000060 M\n"
     ]
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "\n",
    "class SelfAttV1(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttV1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 一般 Linear 都是默认有 bias\n",
    "        # 一般来说， input dim 的 hidden dim\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape is: (batch, seq_len, hidden_dim)， 一般是和 hidden_dim 相同\n",
    "        # 但是 X 的 final dim 可以和 hidden_dim 不同\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        # shape is: (batch, seq_len, seq_len)\n",
    "        # torch.matmul 可以改成 Q @ K.T\n",
    "        # 其中 K 需要改成 shape 为： (batch, hidden_dim, seq_len)\n",
    "        # attention_value = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        attention_value = Q @ K.transpose(-1, -2)\n",
    "        # print(attention_value, attention_value.shape)\n",
    "        attention_wight = torch.softmax(\n",
    "            attention_value / math.sqrt(self.hidden_dim), dim=-1\n",
    "        )\n",
    "        print(\"attention_weight:\", attention_wight)\n",
    "        # shape is: (batch, seq_len, hidden_dim)\n",
    "        output = attention_wight @ V\n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "net = SelfAttV1(4)\n",
    "net(X)\n",
    "# 计算 SelfAttV1 的 GFLOPs\n",
    "flops_v1, params_v1 = profile(net, inputs=(X,))\n",
    "print(f\"SelfAttV1 FLOPs: {flops_v1 / 1e6:.6f} MFLOPs\")\n",
    "print(f\"SelfAttV1 Parameters: {params_v1 / 1e6:.6f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参数量：\n",
    "\n",
    "每个线性层 nn.Linear(hidden_dim, hidden_dim) 有 hidden_dim * hidden_dim + hidden_dim 个参数（权重和偏置）。\n",
    "总共有三个这样的线性层，因此总参数量为 3 * (hidden_dim * hidden_dim + hidden_dim)。\n",
    "\n",
    "- 计算量：\n",
    "\n",
    "Q = self.query_proj(X), K = self.key_proj(X), V = self.value_proj(X)：每个操作涉及 batch_size * seq_len * hidden_dim * hidden_dim 次乘法和 batch_size * seq_len * hidden_dim 次加法。\n",
    "attention_value = Q @ K.transpose(-1, -2)：涉及 batch_size * seq_len * seq_len * hidden_dim 次乘法和 batch_size * seq_len * seq_len * (hidden_dim - 1) 次加法。\n",
    "output = torch.matmul(attention_wight, V)：涉及 batch_size * seq_len * hidden_dim * seq_len 次乘法和 batch_size * seq_len * hidden_dim * (seq_len - 1) 次加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 2, 4)\n",
    "print(X.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  QKV 矩阵计算的时候，可以合并成一个大矩阵计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "SelfAttV1 FLOPs: 0.000384 MFLOPs\n",
      "SelfAttV1 Parameters: 0.000080 M\n"
     ]
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "\n",
    "class SelfAttV2(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # 如果模型比较小，类似于以前的bert 这种size，一个卡就能放下的，用一个大矩阵应该快于三个小矩阵 for 循环（减少读写）\n",
    "        # 但是现在是因为 llm 的tensor 已经够大了，所以会做切分，所以合成大矩阵没有意义了 \n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape is: (batch, seq, dim)\n",
    "\n",
    "        QKV = self.proj(X)  # (batch, seq, dim * 3)\n",
    "        # reshape 从希望的 q, k, 的形式\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "\n",
    "        # print(x)\n",
    "        att_weight = torch.softmax(\n",
    "            Q @ K.transpose(-1, -2) / math.sqrt(self.dim), dim=-1\n",
    "        )\n",
    "        output = att_weight @ V\n",
    "        return self.output_proj(output)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "net = SelfAttV2(4)\n",
    "net(X).shape\n",
    "# 计算 SelfAttV1 的 GFLOPs\n",
    "flops_v2, params_v2 = profile(net, inputs=(X,))\n",
    "print(f\"SelfAttV1 FLOPs: {flops_v2 / 1e6:.6f} MFLOPs\")\n",
    "print(f\"SelfAttV1 Parameters: {params_v2 / 1e6:.6f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入 dropout 、 attention_mask 、 output_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [1, 0, 0, 0],\n",
      "         [1, 0, 0, 0],\n",
      "         [1, 0, 0, 0]]]) torch.Size([3, 4, 4])\n",
      "tensor([[[0.3324, 0.3369, 0.3307, 0.0000],\n",
      "         [0.3390, 0.3296, 0.3315, 0.0000],\n",
      "         [0.3435, 0.3298, 0.3267, 0.0000],\n",
      "         [0.3307, 0.3395, 0.3299, 0.0000]],\n",
      "\n",
      "        [[0.4698, 0.5302, 0.0000, 0.0000],\n",
      "         [0.4999, 0.5001, 0.0000, 0.0000],\n",
      "         [0.4922, 0.5078, 0.0000, 0.0000],\n",
      "         [0.4609, 0.5391, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "\n",
    "class SelfAttV3(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # 这样可以进行加速\n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "        # 一般是 0.1 的 dropout，一般写作 config.attention_probs_dropout_prob\n",
    "        # hidden_dropout_prob 一般也是 0.1\n",
    "        self.att_drop = nn.Dropout(0.1)\n",
    "\n",
    "        # 不写这个应该也没人怪，应该好像是 MultiHeadAttention 中的产物，这个留给 MultiHeadAttention 也没有问题；\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # attention_mask shape is: (batch, seq)\n",
    "        # X shape is: (batch, seq, dim)\n",
    "\n",
    "        QKV = self.proj(X)  # (batch, seq, dim * 3)\n",
    "        # reshape 从希望的 q, k, 的形式\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "\n",
    "        att_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None:\n",
    "            # 给 weight 填充一个极小的值\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        print(att_weight)\n",
    "\n",
    "        # 注意：dropout 是对 att_weight 的，不是对 att_weight @ V 的\n",
    "        att_weight = self.att_drop(att_weight)\n",
    "\n",
    "        output = att_weight @ V\n",
    "        ret = self.output_proj(output)\n",
    "        return ret\n",
    "\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "print(b.shape)\n",
    "# 在第 0 维重复 1 次，在第 1 维重复 4 次，在第 2 维重复 1 次\n",
    "# [3, 4] -> [3, 1, 4] -> [3, 4, 4]]\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(mask, mask.shape)\n",
    "\n",
    "net = SelfAttV3(2)\n",
    "net(X, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 面试写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_weight:\n",
      " tensor([[[0.3460, 0.3163, 0.3377, 0.0000],\n",
      "         [0.3147, 0.3550, 0.3303, 0.0000],\n",
      "         [0.3287, 0.3368, 0.3345, 0.0000],\n",
      "         [0.3105, 0.3601, 0.3294, 0.0000]],\n",
      "\n",
      "        [[0.4935, 0.5065, 0.0000, 0.0000],\n",
      "         [0.4896, 0.5104, 0.0000, 0.0000],\n",
      "         [0.4887, 0.5113, 0.0000, 0.0000],\n",
      "         [0.4988, 0.5012, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "class SelfAttV4(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # 这样很清晰\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "        # 一般是 0.1 的 dropout，一般写作 config.attention_probs_dropout_prob\n",
    "        # hidden_dropout_prob 一般也是 0.1\n",
    "        self.att_drop = nn.Dropout(0.1)\n",
    "\n",
    "        # 可以不写；具体和面试官沟通。\n",
    "        # 这是 MultiHeadAttention 中的产物，这个留给 MultiHeadAttention 也没有问题；\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # attention_mask shape is: (batch, seq)\n",
    "        # X shape is: (batch, seq, dim)\n",
    "\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        att_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None:\n",
    "            # 给 weight 填充一个极小的值\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        print(f\"att_weight:\\n {att_weight}\")\n",
    "\n",
    "        # 注意：dropout 是对 att_weight 的，不是对 att_weight @ V 的\n",
    "        att_weight = self.att_drop(att_weight)\n",
    "\n",
    "        output = att_weight @ V\n",
    "        ret = self.output_proj(output)\n",
    "        return ret\n",
    "\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "# print(b.shape)\n",
    "# 在第 0 维重复 1 次，在第 1 维重复 4 次，在第 2 维重复 1 次\n",
    "# [3, 4] -> [3, 1, 4] -> [3, 4, 4]]\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "\n",
    "net = SelfAttV4(2)\n",
    "net(X, mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: torch.Size([3, 2, 128])\n",
      "q_state: torch.Size([3, 8, 2, 16])\n",
      "attention_weight: torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 8, 2, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head) -> None:\n",
    "        super().__init__()\n",
    "        self.nums_head = nums_head\n",
    "\n",
    "        # 一般来说，\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 一般默认有 bias，需要时刻主意，hidden_dim = head_dim * nums_head，所以最终是可以算成是 n 个矩阵\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # gpt2 和 bert 类都有，但是 llama 其实没有\n",
    "        self.att_dropout = nn.Dropout(0.1)\n",
    "        # 输出时候的 proj\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # 需要在 mask 之前 masked_fill\n",
    "        # X shape is (batch, seq, hidden_dim)\n",
    "        # attention_mask shape is (batch, seq)\n",
    "\n",
    "        batch_size, seq_len, hidden_dim = X.size()\n",
    "\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        print(f\"Q: {Q.shape}\")\n",
    "        # （batch_size, seq_len, num_head, head_dim）->（batch_size, num_head, seq_len, head_dim）\n",
    "        q_state = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        print(f\"q_state: {q_state.shape}\")\n",
    "        k_state = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        v_state = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        # 主意这里需要用 head_dim，而不是 hidden_dim\n",
    "        attention_weight = (\n",
    "            q_state @ k_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        )\n",
    "        print(f\"attention_weight:\", attention_weight.shape)\n",
    "        # print(type(attention_mask))\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float(\"-1e20\")\n",
    "            )\n",
    "\n",
    "        # 第四个维度 softmax\n",
    "        # （batch_size, num_head, seq_len, seq_len）\n",
    "        attention_weight = torch.softmax(attention_weight, dim=3)\n",
    "        # print(attention_weight, attention_weight)\n",
    "\n",
    "        # 注意：dropout 是对 att_weight 的，不是对 att_weight @ V 的\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "        output_mid = attention_weight @ v_state\n",
    "        print(output_mid.shape)\n",
    "\n",
    "        # 重新变成 (batch, seq_len, num_head, head_dim)\n",
    "        # 这里的 contiguous() 是相当于返回一个连续内存的 tensor，一般用了 permute/tranpose 都要这么操作\n",
    "        # 如果后面用 Reshape 就可以不用这个 contiguous()，因为 view 只能在连续内存中操作\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "\n",
    "        # 变成 (batch, seq, hidden_dim),\n",
    "        output = output_mid.view(batch_size, seq_len, -1)\n",
    "        output = self.o_proj(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# attention_mask = (\n",
    "#     torch.tensor(\n",
    "#         [\n",
    "#             [0, 1],\n",
    "#             [0, 0],\n",
    "#             [1, 0],\n",
    "#         ]\n",
    "#     )\n",
    "#     .unsqueeze(1)\n",
    "#     .unsqueeze(2)\n",
    "#     .expand(3, 8, 2, 2)\n",
    "# )\n",
    "attention_mask = torch.randint(0, 2, (3, 8, 2, 2))\n",
    "\n",
    "x = torch.rand(3, 2, 128)\n",
    "net = MultiHeadAttention(128, 8)\n",
    "net(x, attention_mask).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([3, 4, 128])\n",
      "K shape: torch.Size([3, 6, 128])\n",
      "V shape: torch.Size([3, 6, 128])\n",
      "attention_mask shape: torch.Size([3, 8, 4, 6])\n",
      "Q after projection: torch.Size([3, 4, 128])\n",
      "K after projection: torch.Size([3, 6, 128])\n",
      "V after projection: torch.Size([3, 6, 128])\n",
      "q_state: torch.Size([3, 8, 4, 16])\n",
      "k_state: torch.Size([3, 8, 6, 16])\n",
      "v_state: torch.Size([3, 8, 6, 16])\n",
      "attention_weight before mask: torch.Size([3, 8, 4, 6])\n",
      "attention_weight after mask: torch.Size([3, 8, 4, 6])\n",
      "attention_weight after softmax: torch.Size([3, 8, 4, 6])\n",
      "output_mid: torch.Size([3, 8, 4, 16])\n",
      "output: torch.Size([3, 4, 128])\n",
      "Output shape: torch.Size([3, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head) -> None:\n",
    "        super().__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 定义线性层来投影查询、键和值\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # 定义dropout层\n",
    "        self.att_dropout = nn.Dropout(0.1)\n",
    "        # 定义输出层\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, Q, K, V, attention_mask=None):\n",
    "        # Q shape is (batch, seq_len_q, hidden_dim)\n",
    "        # K shape is (batch, seq_len_k, hidden_dim)\n",
    "        # V shape is (batch, seq_len_k, hidden_dim)\n",
    "        # attention_mask shape is (batch, seq_len_q, seq_len_k)\n",
    "\n",
    "        batch_size, seq_len_q, _ = Q.size()\n",
    "        _, seq_len_k, _ = K.size()\n",
    "\n",
    "        # 投影查询、键和值\n",
    "        Q = self.q_proj(Q)\n",
    "        K = self.k_proj(K)\n",
    "        V = self.v_proj(V)\n",
    "\n",
    "        print(f\"Q after projection: {Q.shape}\")\n",
    "        print(f\"K after projection: {K.shape}\")\n",
    "        print(f\"V after projection: {V.shape}\")\n",
    "\n",
    "        # 将查询、键和值重塑为多头形式\n",
    "        q_state = Q.view(batch_size, seq_len_q, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch_size, seq_len_k, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch_size, seq_len_k, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        print(f\"q_state: {q_state.shape}\")\n",
    "        print(f\"k_state: {k_state.shape}\")\n",
    "        print(f\"v_state: {v_state.shape}\")\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attention_weight = (q_state @ k_state.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        print(f\"attention_weight before mask: {attention_weight.shape}\")\n",
    "\n",
    "        # 应用注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        print(f\"attention_weight after mask: {attention_weight.shape}\")\n",
    "\n",
    "        # 对注意力权重进行softmax操作\n",
    "        attention_weight = torch.softmax(attention_weight, dim=-1)\n",
    "\n",
    "        print(f\"attention_weight after softmax: {attention_weight.shape}\")\n",
    "\n",
    "        # 应用dropout\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "\n",
    "        # 计算输出\n",
    "        output_mid = attention_weight @ v_state\n",
    "\n",
    "        print(f\"output_mid: {output_mid.shape}\")\n",
    "\n",
    "        # 重塑输出为(batch, seq_len_q, hidden_dim)\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        output = output_mid.view(batch_size, seq_len_q, -1)\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        print(f\"output: {output.shape}\")\n",
    "\n",
    "        return output\n",
    "\n",
    "# 测试用例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义输入序列 \n",
    "    # Cross-Attention中，查询（Query）通常来自于一个序列（如文本序列），\n",
    "    # 而键（Key）和值（Value）来自于另一个序列（如另一个文本序列或图像特征）\n",
    "    Q = torch.rand(3, 4, 128)  # 查询序列，形状为 (batch_size, seq_len_q, hidden_dim)\n",
    "    K = torch.rand(3, 6, 128)  # 键序列，形状为 (batch_size, seq_len_k, hidden_dim)\n",
    "    V = torch.rand(3, 6, 128)  # 值序列，形状为 (batch_size, seq_len_k, hidden_dim)\n",
    "\n",
    "    # 定义注意力掩码\n",
    "    attention_mask = torch.randint(0, 2, (3, 8, 4, 6))\n",
    "\n",
    "    print(f\"Q shape: {Q.shape}\")\n",
    "    print(f\"K shape: {K.shape}\")\n",
    "    print(f\"V shape: {V.shape}\")\n",
    "    print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "\n",
    "    # 创建CrossAttention实例\n",
    "    cross_attention = CrossAttention(hidden_dim=128, nums_head=8)\n",
    "\n",
    "    # 前向传播\n",
    "    output = cross_attention(Q, K, V, attention_mask)\n",
    "\n",
    "    # 打印输出形状以验证实现是否正确\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head) -> None:\n",
    "        super().__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 定义线性层来投影查询、键和值\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # 定义dropout层\n",
    "        self.att_dropout = nn.Dropout(0.1)\n",
    "        # 定义输出层\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        # Q shape is (batch, seq_len_q, hidden_dim)\n",
    "        # K shape is (batch, seq_len_k, hidden_dim)\n",
    "        # V shape is (batch, seq_len_k, hidden_dim)\n",
    "        # attention_mask shape is (batch, seq_len_q, seq_len_k)\n",
    "\n",
    "        batch_size, seq_len_q, _ = Q.size()\n",
    "        _, seq_len_k, _ = K.size()\n",
    "\n",
    "        # 投影查询、键和值\n",
    "        Q = self.q_proj(Q)\n",
    "        K = self.k_proj(K)\n",
    "        V = self.v_proj(V)\n",
    "\n",
    "        # 将查询、键和值重塑为多头形式\n",
    "        q_state = Q.view(batch_size, seq_len_q, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "        k_state = K.view(batch_size, seq_len_k, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "        v_state = V.view(batch_size, seq_len_k, self.nums_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attention_weight = (q_state @ k_state.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # 对注意力权重进行softmax操作\n",
    "        attention_weight = torch.softmax(attention_weight, dim=-1)\n",
    "\n",
    "        # 应用dropout\n",
    "        attention_weight = self.att_dropout(attention_weight)\n",
    "\n",
    "        # 计算输出\n",
    "        output_mid = attention_weight @ v_state\n",
    "\n",
    "        # 重塑输出为(batch, seq_len_q, hidden_dim)\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        output = output_mid.view(batch_size, seq_len_q, -1)\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# 测试用例\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义输入序列 \n",
    "    # Cross-Attention中，查询（Query）通常来自于一个序列（如文本序列），\n",
    "    # 而键（Key）和值（Value）来自于另一个序列（如另一个文本序列或图像特征）\n",
    "    Q = torch.rand(3, 4, 128)  # 查询序列，形状为 (batch_size, seq_len_q, hidden_dim)\n",
    "    K = torch.rand(3, 6, 128)  # 键序列，形状为 (batch_size, seq_len_k, hidden_dim)\n",
    "    V = torch.rand(3, 6, 128)  # 值序列，形状为 (batch_size, seq_len_k, hidden_dim)\n",
    "\n",
    "    # 创建CrossAttention实例\n",
    "    cross_attention = CrossAttention(hidden_dim=128, nums_head=8)\n",
    "\n",
    "    # 前向传播\n",
    "    output = cross_attention(Q, K, V)\n",
    "\n",
    "    # 打印输出形状以验证实现是否正确\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5_frog38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
